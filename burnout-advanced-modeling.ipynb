{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-01-17T19:27:30.868984Z",
     "iopub.status.busy": "2021-01-17T19:27:30.868260Z",
     "iopub.status.idle": "2021-01-17T19:27:30.914281Z",
     "shell.execute_reply": "2021-01-17T19:27:30.913713Z"
    },
    "papermill": {
     "duration": 0.075842,
     "end_time": "2021-01-17T19:27:30.914418",
     "exception": false,
     "start_time": "2021-01-17T19:27:30.838576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/glove6b100dtxt/glove.6B.100d.txt\n",
      "/kaggle/input/burnout-tweets-analysis/burnout.csv\n",
      "/kaggle/input/burnout-tweets-analysis/__results__.html\n",
      "/kaggle/input/burnout-tweets-analysis/df.csv\n",
      "/kaggle/input/burnout-tweets-analysis/__notebook__.ipynb\n",
      "/kaggle/input/burnout-tweets-analysis/__output__.json\n",
      "/kaggle/input/burnout-tweets-analysis/custom.css\n",
      "/kaggle/input/burnout-tweets-analysis/__results___files/__results___60_0.png\n",
      "/kaggle/input/burnout-tweets-analysis/__results___files/__results___58_0.png\n",
      "/kaggle/input/burnout-tweets-analysis/__results___files/__results___62_0.png\n",
      "/kaggle/input/burnout-tweets-analysis/__results___files/__results___50_0.png\n",
      "/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T19:27:30.982164Z",
     "iopub.status.busy": "2021-01-17T19:27:30.981411Z",
     "iopub.status.idle": "2021-01-17T19:27:38.693635Z",
     "shell.execute_reply": "2021-01-17T19:27:38.694629Z"
    },
    "papermill": {
     "duration": 7.757378,
     "end_time": "2021-01-17T19:27:38.694787",
     "exception": false,
     "start_time": "2021-01-17T19:27:30.937409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "import re\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "stop=set(stopwords.words('english'))\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import (LSTM, \n",
    "                          Embedding, \n",
    "                          BatchNormalization,\n",
    "                          Dense, \n",
    "                          TimeDistributed, \n",
    "                          Dropout, \n",
    "                          Bidirectional,\n",
    "                          Flatten, \n",
    "                          GlobalMaxPool1D)\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import (\n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    confusion_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T19:27:38.750128Z",
     "iopub.status.busy": "2021-01-17T19:27:38.749016Z",
     "iopub.status.idle": "2021-01-17T19:27:39.680370Z",
     "shell.execute_reply": "2021-01-17T19:27:39.679040Z"
    },
    "papermill": {
     "duration": 0.963002,
     "end_time": "2021-01-17T19:27:39.680521",
     "exception": false,
     "start_time": "2021-01-17T19:27:38.717519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.core import Activation\n",
    "\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022465,
     "end_time": "2021-01-17T19:27:39.726044",
     "exception": false,
     "start_time": "2021-01-17T19:27:39.703579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Recalculating CountVecotrizer Vector representation of Tweet Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T19:27:39.781237Z",
     "iopub.status.busy": "2021-01-17T19:27:39.780549Z",
     "iopub.status.idle": "2021-01-17T19:27:39.816850Z",
     "shell.execute_reply": "2021-01-17T19:27:39.816229Z"
    },
    "papermill": {
     "duration": 0.067719,
     "end_time": "2021-01-17T19:27:39.816985",
     "exception": false,
     "start_time": "2021-01-17T19:27:39.749266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/burnout-tweets-analysis/df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T19:27:39.877671Z",
     "iopub.status.busy": "2021-01-17T19:27:39.876951Z",
     "iopub.status.idle": "2021-01-17T19:27:39.885827Z",
     "shell.execute_reply": "2021-01-17T19:27:39.885017Z"
    },
    "papermill": {
     "duration": 0.044808,
     "end_time": "2021-01-17T19:27:39.885990",
     "exception": false,
     "start_time": "2021-01-17T19:27:39.841182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Split data into training, validation and test sets\n",
    "y=df['burnout']\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(df['Tweet Text'].values, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T19:27:39.942371Z",
     "iopub.status.busy": "2021-01-17T19:27:39.941284Z",
     "iopub.status.idle": "2021-01-17T19:27:39.945619Z",
     "shell.execute_reply": "2021-01-17T19:27:39.944933Z"
    },
    "papermill": {
     "duration": 0.033285,
     "end_time": "2021-01-17T19:27:39.945743",
     "exception": false,
     "start_time": "2021-01-17T19:27:39.912458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1503,)\n",
      "(376,)\n"
     ]
    }
   ],
   "source": [
    "print (xtrain.shape)\n",
    "print (xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T19:27:40.032418Z",
     "iopub.status.busy": "2021-01-17T19:27:40.027258Z",
     "iopub.status.idle": "2021-01-17T19:27:40.236682Z",
     "shell.execute_reply": "2021-01-17T19:27:40.235882Z"
    },
    "papermill": {
     "duration": 0.267328,
     "end_time": "2021-01-17T19:27:40.236852",
     "exception": false,
     "start_time": "2021-01-17T19:27:39.969524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english')\n",
    "\n",
    "\n",
    "\n",
    "# Fitting Count Vectorizer to training set\n",
    "ctv.fit(xtrain)\n",
    "xtrain_ctv =  ctv.transform(xtrain) \n",
    "xtest_ctv = ctv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T19:27:40.308500Z",
     "iopub.status.busy": "2021-01-17T19:27:40.297830Z",
     "iopub.status.idle": "2021-01-17T19:27:40.479682Z",
     "shell.execute_reply": "2021-01-17T19:27:40.479048Z"
    },
    "papermill": {
     "duration": 0.216438,
     "end_time": "2021-01-17T19:27:40.479806",
     "exception": false,
     "start_time": "2021-01-17T19:27:40.263368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(xtrain)\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xtest_tfv = tfv.transform(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true,
    "papermill": {
     "duration": 0.02342,
     "end_time": "2021-01-17T19:27:40.526926",
     "exception": false,
     "start_time": "2021-01-17T19:27:40.503506",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As a reminder, for our initial model pass, XGBoost using CountVectorizer's vector ranked the highest in model performance. \n",
    "\n",
    "Now, we will try and extend model performance by trying out some more methods to optimize our model's burnout predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023877,
     "end_time": "2021-01-17T19:27:40.574475",
     "exception": false,
     "start_time": "2021-01-17T19:27:40.550598",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Thank you: https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022972,
     "end_time": "2021-01-17T19:27:40.620805",
     "exception": false,
     "start_time": "2021-01-17T19:27:40.597833",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022827,
     "end_time": "2021-01-17T19:27:40.666958",
     "exception": false,
     "start_time": "2021-01-17T19:27:40.644131",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "From skikit-learn: \n",
    "Grid Search is an exhaustive search over specified parameter values for an estimator.\n",
    "\n",
    "Important members are fit, predict.\n",
    "\n",
    "GridSearchCV implements a “fit” and a “score” method. It also implements “score_samples”, “predict”, “predict_proba”, “decision_function”, “transform” and “inverse_transform” if they are implemented in the estimator used.\n",
    "\n",
    "The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022848,
     "end_time": "2021-01-17T19:27:40.713375",
     "exception": false,
     "start_time": "2021-01-17T19:27:40.690527",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this section, I'll talk about grid search using logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T19:27:40.766651Z",
     "iopub.status.busy": "2021-01-17T19:27:40.765915Z",
     "iopub.status.idle": "2021-01-17T19:27:40.769473Z",
     "shell.execute_reply": "2021-01-17T19:27:40.768868Z"
    },
    "papermill": {
     "duration": 0.033044,
     "end_time": "2021-01-17T19:27:40.769606",
     "exception": false,
     "start_time": "2021-01-17T19:27:40.736562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize SVD\n",
    "svd = TruncatedSVD()\n",
    "    \n",
    "# Initialize the standard scaler \n",
    "scl = preprocessing.StandardScaler()\n",
    "\n",
    "# We will use logistic regression here..\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('svd', svd),\n",
    "                         ('scl', scl),\n",
    "                         ('lr', lr_model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023714,
     "end_time": "2021-01-17T19:27:40.817083",
     "exception": false,
     "start_time": "2021-01-17T19:27:40.793369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Grid of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T19:27:40.869944Z",
     "iopub.status.busy": "2021-01-17T19:27:40.869275Z",
     "iopub.status.idle": "2021-01-17T19:27:40.871726Z",
     "shell.execute_reply": "2021-01-17T19:27:40.872334Z"
    },
    "papermill": {
     "duration": 0.031739,
     "end_time": "2021-01-17T19:27:40.872492",
     "exception": false,
     "start_time": "2021-01-17T19:27:40.840753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = {'svd__n_components' : [120, 180],\n",
    "              'lr__C': [0.1, 1.0, 10], \n",
    "              'lr__penalty': ['l1', 'l2']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024315,
     "end_time": "2021-01-17T19:27:40.920904",
     "exception": false,
     "start_time": "2021-01-17T19:27:40.896589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So, for SVD we evaluate 120 and 180 components and for logistic regression we evaluate three different values of C with l1 and l2 penalty. We can now start grid search on these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T19:27:40.972766Z",
     "iopub.status.busy": "2021-01-17T19:27:40.972120Z",
     "iopub.status.idle": "2021-01-17T19:27:40.977754Z",
     "shell.execute_reply": "2021-01-17T19:27:40.977170Z"
    },
    "papermill": {
     "duration": 0.032196,
     "end_time": "2021-01-17T19:27:40.977887",
     "exception": false,
     "start_time": "2021-01-17T19:27:40.945691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Defining scoring metric\n",
    "f1 = make_scorer(f1_score , pos_label=0, average='binary') #making sure f1 score is catching minority class (non-burnout class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T19:27:41.032934Z",
     "iopub.status.busy": "2021-01-17T19:27:41.032272Z",
     "iopub.status.idle": "2021-01-17T19:27:41.035144Z",
     "shell.execute_reply": "2021-01-17T19:27:41.035622Z"
    },
    "papermill": {
     "duration": 0.033383,
     "end_time": "2021-01-17T19:27:41.035772",
     "exception": false,
     "start_time": "2021-01-17T19:27:41.002389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=f1,\n",
    "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T19:27:41.093196Z",
     "iopub.status.busy": "2021-01-17T19:27:41.092484Z",
     "iopub.status.idle": "2021-01-17T19:27:57.454128Z",
     "shell.execute_reply": "2021-01-17T19:27:57.454841Z"
    },
    "papermill": {
     "duration": 16.395195,
     "end_time": "2021-01-17T19:27:57.455061",
     "exception": false,
     "start_time": "2021-01-17T19:27:41.059866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:   12.9s remaining:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:   14.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.539\n",
      "Best parameters set:\n",
      "\tlr__C: 10\n",
      "\tlr__penalty: 'l2'\n",
      "\tsvd__n_components: 180\n"
     ]
    }
   ],
   "source": [
    "# Fit Grid Search Model using Count Vect\n",
    "model.fit(xtrain_ctv, ytrain)  # we can use the full data here but im only using xtrain\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035283,
     "end_time": "2021-01-17T19:27:57.522619",
     "exception": false,
     "start_time": "2021-01-17T19:27:57.487336",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The score is still the second to the worst of minority class' f1 scores from all prior training runs.\n",
    "\n",
    "Let's see what TFIDF produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T19:27:57.598802Z",
     "iopub.status.busy": "2021-01-17T19:27:57.597746Z",
     "iopub.status.idle": "2021-01-17T19:27:58.644919Z",
     "shell.execute_reply": "2021-01-17T19:27:58.644375Z"
    },
    "papermill": {
     "duration": 1.091106,
     "end_time": "2021-01-17T19:27:58.645040",
     "exception": false,
     "start_time": "2021-01-17T19:27:57.553934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0877s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of  24 | elapsed:    0.6s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:    0.8s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.560\n",
      "Best parameters set:\n",
      "\tlr__C: 10\n",
      "\tlr__penalty: 'l2'\n",
      "\tsvd__n_components: 180\n"
     ]
    }
   ],
   "source": [
    "# Fit Grid Search Model using TFIDF\n",
    "model.fit(xtrain_tfv, ytrain)  \n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028644,
     "end_time": "2021-01-17T19:27:58.704265",
     "exception": false,
     "start_time": "2021-01-17T19:27:58.675621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "TFIDF actually did a little better. So let's move onto grid search on higher performing models - such as NB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T19:27:58.774778Z",
     "iopub.status.busy": "2021-01-17T19:27:58.773857Z",
     "iopub.status.idle": "2021-01-17T19:27:58.860022Z",
     "shell.execute_reply": "2021-01-17T19:27:58.859097Z"
    },
    "papermill": {
     "duration": 0.127226,
     "end_time": "2021-01-17T19:27:58.860191",
     "exception": false,
     "start_time": "2021-01-17T19:27:58.732965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "Best score: 0.583\n",
      "Best parameters set:\n",
      "\tnb__alpha: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0113s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0113s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Grid Search on TFIDF\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('nb', nb_model)])\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=f1,\n",
    "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(xtrain_tfv, ytrain)  # we can use the full data here but im only using xtrain. \n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030057,
     "end_time": "2021-01-17T19:27:58.920576",
     "exception": false,
     "start_time": "2021-01-17T19:27:58.890519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Great! This is a better result  than what we got with Log Reg. Let us try Grid Search using Count Vect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T19:27:59.004729Z",
     "iopub.status.busy": "2021-01-17T19:27:58.990478Z",
     "iopub.status.idle": "2021-01-17T19:27:59.080137Z",
     "shell.execute_reply": "2021-01-17T19:27:59.080770Z"
    },
    "papermill": {
     "duration": 0.130359,
     "end_time": "2021-01-17T19:27:59.080913",
     "exception": false,
     "start_time": "2021-01-17T19:27:58.950554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "Best score: 0.610\n",
      "Best parameters set:\n",
      "\tnb__alpha: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0149s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Grid Search on Count Vect\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('nb', nb_model)])\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=f1,\n",
    "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(xtrain_ctv, ytrain)  # we can use the full data here but im only using xtrain. \n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030772,
     "end_time": "2021-01-17T19:27:59.142752",
     "exception": false,
     "start_time": "2021-01-17T19:27:59.111980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Much better! (Though same result prior to grid search. So grid search can be advantageous at times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030714,
     "end_time": "2021-01-17T19:27:59.204628",
     "exception": false,
     "start_time": "2021-01-17T19:27:59.173914",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031216,
     "end_time": "2021-01-17T19:27:59.266969",
     "exception": false,
     "start_time": "2021-01-17T19:27:59.235753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Another way to improve our model is to work on the actual data itself. \n",
    "\n",
    "* Count Vectorizer just counts occurrences of words which puts bias on most frequent words and loses rare words. Though data processing is efficient. \n",
    "\n",
    "* Tf-Idf helps with bias against most frequent words. Weights are put on each word. There is penalization of frequent words. \n",
    "\n",
    "* Word2Vec treats each word in corpus like an atomic entity and generates a vector for each word. In this sense Word2vec is very much like Glove - both treat words as the smallest unit to train on. \n",
    "\n",
    "* Fasttext (which is essentially an extension of word2vec model), treats each word as composed of character ngrams. So the vector for a word is made of the sum of this character n grams. For example the word vector “apple” is a sum of the vectors of the n-grams “<ap”, “app”, ”appl”, ”apple”, ”apple>”, “ppl”, “pple”, ”pple>”, “ple”, ”ple>”, ”le>” (assuming hyperparameters for smallest ngram[minn] is 3 and largest ngram[maxn] is 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T19:27:59.337288Z",
     "iopub.status.busy": "2021-01-17T19:27:59.336501Z",
     "iopub.status.idle": "2021-01-17T19:33:33.422978Z",
     "shell.execute_reply": "2021-01-17T19:33:33.422179Z"
    },
    "papermill": {
     "duration": 334.124295,
     "end_time": "2021-01-17T19:33:33.423365",
     "exception": false,
     "start_time": "2021-01-17T19:27:59.299070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196018it [05:34, 6573.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196017 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('../input/glove840b300dtxt/glove.840B.300d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split(\" \")\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T19:33:35.639705Z",
     "iopub.status.busy": "2021-01-17T19:33:35.639039Z",
     "iopub.status.idle": "2021-01-17T19:33:35.642582Z",
     "shell.execute_reply": "2021-01-17T19:33:35.642044Z"
    },
    "papermill": {
     "duration": 1.109845,
     "end_time": "2021-01-17T19:33:35.642702",
     "exception": false,
     "start_time": "2021-01-17T19:33:34.532857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this function creates a normalized vector for the whole sentence\n",
    "\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T19:33:37.864475Z",
     "iopub.status.busy": "2021-01-17T19:33:37.863715Z",
     "iopub.status.idle": "2021-01-17T19:33:38.803017Z",
     "shell.execute_reply": "2021-01-17T19:33:38.801933Z"
    },
    "papermill": {
     "duration": 2.045141,
     "end_time": "2021-01-17T19:33:38.803191",
     "exception": false,
     "start_time": "2021-01-17T19:33:36.758050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1503/1503 [00:00<00:00, 2018.92it/s]\n",
      "100%|██████████| 376/376 [00:00<00:00, 2061.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# create sentence vectors using the above function for training and test set\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "xtest_glove = [sent2vec(x) for x in tqdm(xtest)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T19:33:41.087693Z",
     "iopub.status.busy": "2021-01-17T19:33:41.086994Z",
     "iopub.status.idle": "2021-01-17T19:33:41.095624Z",
     "shell.execute_reply": "2021-01-17T19:33:41.095009Z"
    },
    "papermill": {
     "duration": 1.170991,
     "end_time": "2021-01-17T19:33:41.095748",
     "exception": false,
     "start_time": "2021-01-17T19:33:39.924757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xtest_glove = np.array(xtest_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.0992,
     "end_time": "2021-01-17T19:33:43.341975",
     "exception": false,
     "start_time": "2021-01-17T19:33:42.242775",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's get back to XGBoost and fit our new GloVe embeddings on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T19:33:45.555831Z",
     "iopub.status.busy": "2021-01-17T19:33:45.554965Z",
     "iopub.status.idle": "2021-01-17T19:33:52.410503Z",
     "shell.execute_reply": "2021-01-17T19:33:52.411731Z"
    },
    "papermill": {
     "duration": 7.964855,
     "end_time": "2021-01-17T19:33:52.411954",
     "exception": false,
     "start_time": "2021-01-17T19:33:44.447099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:33:45] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.57      0.63       136\n",
      "           1       0.78      0.86      0.82       240\n",
      "\n",
      "    accuracy                           0.76       376\n",
      "   macro avg       0.74      0.72      0.73       376\n",
      "weighted avg       0.75      0.76      0.75       376\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(nthread=10, silent=False)\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict(xtest_glove)\n",
    "\n",
    "print (classification_report(ytest, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-17T19:33:54.769847Z",
     "iopub.status.busy": "2021-01-17T19:33:54.768818Z",
     "iopub.status.idle": "2021-01-17T19:33:54.778083Z",
     "shell.execute_reply": "2021-01-17T19:33:54.777422Z"
    },
    "papermill": {
     "duration": 1.125237,
     "end_time": "2021-01-17T19:33:54.778196",
     "exception": false,
     "start_time": "2021-01-17T19:33:53.652959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 78,  58],\n",
       "       [ 33, 207]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(ytest, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.11968,
     "end_time": "2021-01-17T19:33:57.006106",
     "exception": false,
     "start_time": "2021-01-17T19:33:55.886426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Awesome! Though confusion matrix shows that GloVe embeddings had more incorrectly classified tweets (false positives and false negatives), Recall and f1 for minority class was highest so far. This is promising. It shows that embeddings are superior than simple word count/frequency vectors. Next up is Deep Learning since this is the modern way of text classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 393.285963,
   "end_time": "2021-01-17T19:33:59.205975",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-17T19:27:25.920012",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
