{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 install tweepy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Link to Twitter Dev Portal: https://developer.twitter.com/en/portal/projects/1342246467465965568/apps/19648638/keys","metadata":{}},{"cell_type":"markdown","source":"Thank you: https://gist.github.com/vickyqian/f70e9ab3910c7c290d9d715491cde44c","metadata":{}},{"cell_type":"code","source":"import tweepy\nimport csv\nimport pandas as pd\n####input your credentials here\nconsumer_key = \nconsumer_secret = \naccess_token = \naccess_token_secret = \n\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\napi = tweepy.API(auth,wait_on_rate_limit=True)\n#####Healthcare Workers Burnout \n# Open/Create a file to append data\ncsvFile = open('burnout.csv', 'a')\n#Use csv Writer\ncsvWriter = csv.writer(csvFile)\n\ncreated_at = []\ntext = []\nfor tweet in tweepy.Cursor(api.search,q=[\"#covidnurse\"],count=100,\n                           lang=\"en\",\n                           since=\"2020-03-15\").items():\n    print (tweet.created_at, tweet.text)\n    csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8')])\n    created_at.append(tweet.created_at)\n    text.append(tweet.text)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df =  pd.DataFrame(\n    {'created_at': created_at,\n     'tweet': text\n    })\ndf #RT (Retweets) are duplicates since Twitter is an echo chamber; will need to get rid of duplicates","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clean = df[~df.tweet.str.contains(\"RT\")]\ndf_clean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 install nlp-profiler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nlp_profiler.core import apply_text_profiling\n\nnew_text_column_dataset = apply_text_profiling(df_clean, 'tweet')\nnew_text_column_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"nlp_profiler is not accurate...will do own EDA and analysis on tweets","metadata":{}},{"cell_type":"markdown","source":"# Scrape multiple users and multiple hashtags/keywords from Twitter","metadata":{}},{"cell_type":"markdown","source":" Thank you: https://github.com/MIA-GH/Elections/blob/master/scripts/main.py","metadata":{}},{"cell_type":"code","source":"\"\"\"\n    This script file extracts tweets from multiple nurses who have commented about their Pandemic experience. \n    Also, several key words/hashtags are used that have had a lot of information about toll the Pandemic has had on healthcare workers. \n\"\"\"\n\nimport tweepy\nfrom tweepy import Cursor\nimport unicodecsv\nfrom unidecode import unidecode\n#from scripts import twitter_credentials as api\n\n# Authentication and connection to Twitter API.\nconsumer_key =\nconsumer_secret = \naccess_key = \naccess_secret = \n\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_key, access_secret)\napi = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n\n\ndef main():\n    # the Script targets any of these keywords in the loop\n    terms = [\n        \"covidnurse\", \"RN\", \"\", \"nursescovid19\", \"nursetwitter\", \"moralinjury\", \"thosewecarry\"\n    ]\n\n    # insert the keyword here for the extraction to continue\n    retweet_filter = '-filter:retweets'\n\n    # append the term to search parameters\n    q = retweet_filter\n    tweets_per_qry = 100\n    since_id = None\n\n    max_id = -1\n    max_tweets = 1000\n\n    tweet_count = 0\n\n    # Usernames whose tweets we want to gather.\n    users = [\n        \"rn_critcare\",\n     \n    ]\n\n    # giving the user some feed back that the script is running\n    print(\"Downloading tweets from user timelines...\")\n\n    # extract tweets from timeline of targeted nurses/hashtags\n    try:\n        with open('burnout2.csv', 'ab') as file:\n            writer = unicodecsv.writer(file, delimiter=',', quotechar='\"')\n            # Write header row.\n\n            # write the titles for each column\n            writer.writerow([\n                \"Tweet Date\",\n                \"Tweet ID\",\n                \"Tweet Text\",\n                \"tweet_source\",\n                \"tweet_retweet_count\",\n                \"tweet_favorite_count\"\n            ])\n\n            # loop through all the users and extract tweets from their relative timelines\n            for user in users:\n\n                # Get 1000 most recent tweets for the current user.\n                for tweet in Cursor(api.user_timeline, screen_name=user).items(1000):\n                    # Get info specific to the current tweet of the current user.\n                    tweet_text = unidecode(tweet.text)\n\n                    # format the date\n                    tweet_date = str(tweet.created_at.year) + \"/\" + str(tweet.created_at.month) + \"/\" + str(\n                        tweet.created_at.day)\n\n                    tweet_source = tweet.source\n\n                    retweet_count = tweet.retweet_count\n\n                    favorite_count = tweet.favorite_count\n\n                    tweet_id = tweet.id\n\n                    # Write the extracted tweets to CSV.\n                    writer.writerow([\n                        tweet_date, tweet_id, tweet_text, tweet_source,\n                        retweet_count, favorite_count,\n                    ])\n\n                # Show progress.\n                print(\"Wrote tweets of %s to CSV.\" % user)\n\n            print(\"streaming at least {0} tweets\".format(max_tweets))\n            for term in terms:\n                search_query = term + q\n                while tweet_count < max_tweets:\n                    try:\n                        if max_id <= 0:\n                            if not since_id:\n                                new_tweets = api.search(q=search_query, lang=\"en\", count=tweets_per_qry,\n                                                        tweet_mode='extended')\n\n                            else:\n                                new_tweets = api.search(q=search_query, lang=\"en\", count=tweets_per_qry,\n                                                        since_id=since_id, tweet_mode='extended')\n                        else:\n                            if not since_id:\n                                new_tweets = api.search(q=search_query, lang=\"en\", count=tweets_per_qry,\n                                                        max_id=str(max_id - 1), tweet_mode='extended')\n                            else:\n                                new_tweets = api.search(q=search_query, lang=\"en\", count=tweets_per_qry,\n                                                        max_id=str(max_id - 1),\n                                                        since_id=since_id, tweet_mode='extended')\n\n                        if not new_tweets:\n                            print(\"No more tweets found\")\n                            break\n                        for tweet in new_tweets:\n                            tweet_text = unidecode(tweet.full_text)\n\n                            tweet_date = str(tweet.created_at.year) + \"/\" + str(tweet.created_at.month) + \"/\" + str(\n                                tweet.created_at.day)\n\n                            tweet_source = tweet.source\n\n                            retweet_count = tweet.retweet_count\n\n                            favorite_count = tweet.favorite_count\n\n                            tweet_id = tweet.id\n                            writer.writerow([\n                                tweet_date, tweet_id, tweet_text, tweet_source,\n                                retweet_count, favorite_count,\n                            ])\n\n                        tweet_count += len(new_tweets)\n                        print(\"Downloaded {0} tweets\".format(tweet_count))\n                        max_id = new_tweets[-1].id\n\n                    except tweepy.TweepError as e:\n                        # Just exit if any error\n                        print(\"some error : \" + str(e))\n                        break\n            print(\"Downloaded at least {0} tweets, Saved to csv file\".format(tweet_count))\n    except tweepy.TweepError as e:\n        print(\"There was an error, find details below, else check your internet connection or your \" +\n              \" credentials in the credentials.py file \\n\")\n        print(\"If this is not your first time running this particular script, then there is a possibility that the \"\n              \"maximum rate limit has been exceeded. wait a few more minutes and re run the script.\\n\")\n        print(f\"Error Details: {str(e)}\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2 = pd.read_csv('burnout2.csv')\ndf2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2['Tweet Text'].str.contains(\"RT\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Retweets are present despite scrape programmed to filter them out. So will do a Pandas filter just to make sure to eliminate redundancy.","metadata":{}},{"cell_type":"code","source":"df_clean2 = df2[~df2['Tweet Text'].str.contains(\"RT\")]\ndf_clean2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clean2.to_csv('df.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great! 1885 rows to use for training and testing an NLP model :)","metadata":{}}]}